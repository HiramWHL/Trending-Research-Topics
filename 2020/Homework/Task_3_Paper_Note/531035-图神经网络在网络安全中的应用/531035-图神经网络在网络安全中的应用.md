    节点复制以防止图神经网络拓扑攻击
    深度学习模型在现实世界系统中的应用越来越普遍，其鲁棒性和抗攻击能力受到越来越多的关注。随着对基于图的机器学习技术兴趣的增加，已经有研究表明，许多深度神经网络容易受到恶意攻击，这已引起人们对其可靠性的严重关注。特别是图的拓扑结构的破坏会严重降低基于图的学习算法的性能。这是由于这些算法的预测能力主要依赖于图连通所施加的相似结构。因此，检测错误的位置和纠正诱导的错误就变得至关重要。最近有一些研究解决了检测问题，但是这些方法并没有解决攻击对downstream task的影响。我认为可以使用节点复制来减轻由对抗性攻击导致的分类下降。
    在许多问题领域，包括推荐系统、欺诈检测、疾病结果和药物相互作用预测，数据项之间存在结构性关系。图是表示这些关系的一种自然机制，这使得人们希望将神经网络的成功转化为图的设置。大量的研究工作已经产生了许多模型和算法。已经证明，图的知识可以用来弥补有限的数据访问。随后，这些型号在工业上得到了成功的应用。这引起了人们对图神经网络弱点的关注，研究人员已经开始开发和研究攻击和防御机制。了解GNN的对抗性弱点有助于暴露现有GNN模型的局限性，并能启发更好的模型和培训策略。
    卷积神经网络通常会受到攻击，包括通过数据操作来改变特征。图神经网络可以受到类似的攻击，但是它们也会受到另一种形式的攻击，这种攻击涉及改变图的拓扑结构。有人提出了Nettack，一种构造图数据的对立扰动的方法，它改变了图的拓扑和节点属性，从而导致节点分类性能的显著下降。其他团队所做实验分析表明，与改变图的特征相比，对图数据拓扑的攻击对分类性能的影响更为严重。试图破坏图中单个节点的分类，会使整个图表的性能恶化。还有人提出了其他针对图的攻击，这些攻击强调了GNNs在更广泛的推理任务中的脆弱性。
    针对图数据学习攻击的发展，已有一些关于检测攻击的初步研究。已经有人提出了算法，通过修改节点的边缘来检测哪些节点受到了攻击。该过程依赖于攻击在被攻击节点的邻域内的分类输出中引发的不一致性。尽管中的技术提供了一种很有前途的方法来检测攻击，但它没有提供一种机制来纠正学习算法的输出。
    所以现在可以着重处理的问题是，当一个检测程序通知我们一个节点很有可能受到拓扑攻击后，应该做什么。可以通过一个复制过程来恢复部分图神经网络对损坏节点的模型精度。整个过程可以包含受攻击节点的特征复制到图中的多个相似位置，并计算这些位置的输出。当特征移动到与它的真实类相对应的位置并且没有受到攻击时，GNN可以返回一个正确的分类。可以将该方法与攻击检测技术相结合，进一步扩展该工作，用一个更真实的场景进行模拟。