# **Adversarial Attacks on Neural Networks for Graph Data**

### 论文笔记作者：魏来 2017141451031

## 论文基本信息

#### Authors：Daniel Zügner Amir Akbarnejad Stephan Günnemann

#### Link：🔗https://dl.acm.org/doi/pdf/10.1145/3219819.3220078

#### From：🔗https://dblp.uni-trier.de/db/conf/kdd/kdd2018.html[ACM Knowledge Discovery and Data Mining(CCF A类推荐会议,KDD)]

## 正文

​		近十年，深度学习成为人工智能和机器学习这顶皇冠上的明珠，在声学、图像和自然语言处理领域展示了顶尖的性能。深度学习提取数据底层复杂模式的表达能力广受认可。但是，现实世界中普遍存在的图数据却是个难点，图可以表示对象及其关系，如社交网络、电商网络、生物网络和交通网络。图也被认为是包含丰富潜在价值的复杂结构。因此，如何利用深度学习方法进行图数据分析近年来吸引了大量的研究者关注。

​		图数据的复杂性对现有机器学习算法提出了重大挑战，因为图数据是不规则的。每张图大小不同、节点无序，一张图中的每个节点都有不同数目的邻近节点，使得一些在图像中容易计算的重要运算（如卷积）不能再直接应用于图。

​		受到卷积网络在计算机视觉领域所获巨大成功的激励，近来出现了很多为图数据重新定义卷积概念的方法，即GCN （图卷积神经网络），如下图所示。GCN 分为两类，分别基于谱和空间。基于谱的方法通过从图信号处理的角度引入滤波器来定义图卷积，其中图卷积运算被解释为从图信号中去除噪声。基于空间的方法将图卷积表征为聚合来自近邻的特征信息。图卷积神经网络实际上跟CNN的作用一样，是一个特征提取器，它的对象是图数据,其能力超出了其非线性、层级本质，依赖于利用图关系信息来执行分类任务：不仅仅独立地考虑实例（节点及其特征），还利用实例之间的关系（边缘）。与CNN相比，GCN具有 **局部参数共享** 、**感受域正比于层数**的性质，能同时对节点特征信息与结构信息进行端对端学习，是目前对图数据学习任务的最佳选择。图卷积适用性极广，适用于任意拓扑结构的节点与图。而且，在节点分类与边预测等任务上，图卷积神经网络在公开数据集上效果要远远优于其他方法。

![](https://pic.downk.cc/item/5ec91cd5c2a9a83be5381f4b.png)

​		图数据是很多高影响力应用的核心，比如社交和评级网络分析（Facebook、Amazon）、基因相互作用网络（BioGRID），以及互连文档集合（PubMed、Arxiv）。图卷积神经网络精妙地设计了一种从图数据中提取特征的方法，从而让我们可以使用这些特征去对图数据进行**节点分类、图分类、边预测**，用途广泛。基于图数据的一个最常应用任务是节点分类：给出一个大的（属性）图和一些节点的类别标签，来预测其余节点的类别标签。

​		然而用于分类学习任务的深度学习架构很容易被欺骗或者攻击[2,3]。即使是添加轻微扰动因素（即对抗扰动／样本）也可能导致结果不直观、不可信，也给想要利用这些缺陷的攻击者开了方便之门。目前基于图的深度学习方法的对抗扰动问题并未得到解决。尤其是对于使用基于图的学习的领域（如 web），虚假数据很容易侵入：比如垃圾邮件制造者向社交网络添加错误的信息；犯罪分子频繁操控在线评论和产品网站。

​		如下图所示，攻击者处于图中的某一节点，通过添加扰动（如与图中的其他节点建立边），就会导致分类算法对于其他节点的分类造成错误。

![](https://pic.downk.cc/item/5ec921e1c2a9a83be53f91f2.png)

​		该论文提出一个对属性图进行对抗扰动的原则，旨在欺骗当前最优的图深度学习模型。该研究主要针对基于图卷积网络的半监督分类模型，但提出的方法也有可能适用于无监督模型 DeepWalk。研究者默认假设攻击者具备全部数据的知识，但只能操纵其中的一部分。但是，即使仅了解部分数据，实验证明研究中的攻击仍然有效。

​		简而言之该论文提出的攻击方式就是在节点的特征和图的结构上，生成微小的**对抗性扰动（adversarial perturbation）**从而迷惑图神经网络，使其无法正常工作，无法将节点进行正确分类。加入的微小扰动是非常微小的，不能改变原始的图。扰动增加后，原图G=(A,X)变成G=(A’,X’)，其中A是邻接矩阵，改变A就是对图的结构进行改变，X是每个节点的特征，改变X就是对图的特征进行改变。

​		在实验中，研究者进一步证明这些结果可迁移至其他模型、不同数据集，甚至在仅可以观察到部分数据时仍然有效。整体而言，这强调了应对图数据攻击的必要性。

​		图卷积神经网络模型已经在节点分类任务上实现了强大的性能。这种方法也被广泛的运用在了社交网络、电商的业务应用中。在这些应用业务中，存在很多对抗性攻击，图卷积神经网络在面对这一类攻击时，具有脆弱性，由此产生了一系列安全问题。面对抵御对抗性攻击，图神经网络技术亟待研究者们进一步展开研究。



### 其他参考文献

[1] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, G. Monfardini, The graph neural network model, Trans. Neural Networks 20(1):61-80, 2009.

[2] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel. 2017. Adversarial Examples for Malware Detection. In European Symposium on Research in Computer Security. 62–79.

[3] Mohamad Ali Torkamani and Daniel Lowd. 2013. Convex adversarial collective classification. In ICML. 642–650. 

